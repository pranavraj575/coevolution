import torch, shutil, os, sys
import numpy as np
import dill as pickle
# from multiprocessing import Pool
from pathos.multiprocessing import Pool

from src.team_trainer import TeamTrainer
from src.game_outcome import PlayerInfo, PettingZooOutcomeFn
from src.zoo_cage import ZooCage
from src.utils.dict_keys import (DICT_AGE,
                                 DICT_MUTATION_AGE,
                                 DICT_CLONABLE,
                                 DICT_MUTATION_REPLACABLE,
                                 DICT_CLONE_REPLACABLE,
                                 DICT_POSITION_DEPENDENT,
                                 DICT_KEEP_OLD_BUFFER,
                                 DICT_UPDATE_WITH_OLD_BUFFER,

                                 DICT_TRAIN,
                                 DICT_IS_WORKER,
                                 DICT_COLLECT_ONLY,
                                 DICT_SAVE_BUFFER,
                                 DICT_SAVE_CLASS,

                                 TEMP_DICT_CAPTIAN,
                                 TEMP_DICT_CAPTIAN_UNIQUE,

                                 COEVOLUTION_DICT_ELOS,
                                 COEVOLUTION_DICT_CAPTIAN_ELO_UPDATE,
                                 COEVOLUTION_DICT_MEMBER_ELO_UPDATE,
                                 COEVOLUTION_DICT_ELO_CONVERSION,
                                 COEVOLUTION_DICT_DEPTH_OF_RETRY,
                                 )
from src.utils.savele_baselines import load_worker

from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm
from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm

from unstable_baselines3.common.common import DumEnv

sys.setrecursionlimit(2000)


def train_episode(pre_episode_dict):
    """
    NOTE: WE DEFINE THIS OUTSIDE CLASSES FOR PARALLELIZATION PURPOSES
        if this is defined inside a class (i.e. self.train_episode)
        then running multiprocess parallelization on this function will need to pass a copy of self into each process
        this can be incredibly slow and also not work
        the solution is to define this function, which takes in a minimal amount of information to save an experiment

    inputs to this function are generated by pre_episode_generation function in the relevant class
    outputs are recieved by the update_results function in relevant class

    the outputs sent can be controlled by what is returned in outcome_fn.pop_local_mem()
        Look in the game_outcome class and decide what should be popped from memory


    takes a choice of team captians and trains them in RL environment
    updates variables to reflect result of game(s)
    Args:
        pre_episode_dict: object passed from pre_episode_generation
    Returns: info to be passed to update_results
    """
    (ident,
     teams,
     agents,
     train_infos,
     env,
     outcome_fn,
     episode_info,
     captian_choices,
     ) = (pre_episode_dict[key] for key in ('ident',
                                            'teams',
                                            'agents',
                                            'train_infos',
                                            'env',
                                            'outcome_fn',
                                            'episode_info',
                                            'captian_choices',
                                            ))
    outcome_fn.set_ident(ident=ident)  # sets a unique save directory
    # collect result
    team_outcomes = outcome_fn.get_outcome(team_choices=teams,
                                           agent_choices=agents,
                                           updated_train_infos=train_infos,
                                           env=env,
                                           )
    episode_info['team_outcomes'] = tuple(t for t, _ in team_outcomes)

    # save things
    # dict contains all info needed to save current run
    items_to_save = {
        'outcome_local_mem': outcome_fn.pop_local_mem(),
        'captian_choices': captian_choices,
        'teams': teams,
        'team_outcomes': team_outcomes,
        'episode_info': episode_info,
    }
    return items_to_save


class CoevolutionBase:
    """
    general coevolution algorithm
    """

    def __init__(self,
                 outcome_fn_gen,
                 population_sizes,
                 team_sizes=(1, 1),
                 member_to_population=None,
                 processes=0,
                 storage_dir=None,
                 ):
        """
        Args:
            outcome_fn_gen: constructs outcome_fn outcomes and do RL training
                ()-> OutcomeFn
            population_sizes: list of number of agents in each population
                usually can just be a list of one element
                multiple populations are useful if there are different 'types' of agents
                    in the game that take different inputs/have access to different actions
            team_sizes: tuple of number of agents in each team
                i.e. (1,1) is a 1v1 game
            processes: if positive, uses multiprocessing
                note that for most display gui stuff, 0 is necessary
            member_to_population: takes team member (team_idx, member_idx) and returns set of
                populations (subset of (0<i<len(population_sizes))) that the member can be drawn from
                by default, assumes each member can be drawn from each population
            storage_dir: place to store files and such
        """
        self.outcome_fn_gen = outcome_fn_gen
        self.population_sizes = population_sizes
        self.team_sizes = team_sizes
        self.num_teams = len(team_sizes)
        self.env_constructor = lambda infos: None

        # cumulative population sizes
        self.cumsums = np.cumsum(np.concatenate(([0], self.population_sizes)))

        # total population size
        self.N = int(self.cumsums[-1])

        if self.N%self.num_teams != 0:
            print("WARNING: number of agents is not divisible by num teams")
            print('\tthis is fine, but will have non-uniform game numbers for each agent')

        self.build_pop_to_member_and_team(member_to_population)
        self.original_member_to_population = member_to_population
        self.info = {'epochs': 0,
                     'epoch_infos': [],
                     }
        self.processes = processes
        self.storage_dir = storage_dir

    def set_storage_dir(self, storage_dir):
        self.storage_dir = storage_dir

    def clear(self):
        """
        clears any disc storage
        """
        pass

    def create_outcome_fn(self):
        """
        creates outcome function using self.outcome_fn_gen
        Returns: OutcomeFn
        """
        return self.outcome_fn_gen()

    def save(self, save_dir):

        if os.path.exists(save_dir):
            shutil.rmtree(save_dir)
        os.makedirs(save_dir)

        f = open(os.path.join(save_dir, 'info.pkl'), 'wb')
        pickle.dump(self.info, f)
        f.close()

    def load(self, save_dir):
        f = open(os.path.join(save_dir, 'info.pkl'), 'rb')
        self.info.update(pickle.load(f))
        f.close()

    def build_pop_to_member_and_team(self, original_member_to_pop):
        if original_member_to_pop is None:
            original_member_to_pop = lambda team_idx, member_idx: set(range(len(self.population_sizes)))
        self.pop_to_team = [set() for _ in self.population_sizes]
        self.pop_to_member = [set() for _ in self.population_sizes]

        self.team_to_pop = [set() for _ in self.team_sizes]
        self.member_to_pop = [[set() for _ in range(team_size)] for team_size in self.team_sizes]

        self.pop_and_team_to_valid_locations = [
            [torch.zeros(team_size, dtype=torch.bool) for team_size in self.team_sizes]
            for _ in self.population_sizes
        ]
        for team_idx, team_size in enumerate(self.team_sizes):
            for member_idx in range(team_size):
                populations = original_member_to_pop(team_idx, member_idx)
                if populations is None:
                    populations = set(range(len(self.population_sizes)))
                for pop_idx in populations:
                    self.pop_to_member[pop_idx].add((team_idx, member_idx))
                    self.pop_to_team[pop_idx].add(team_idx)

                    self.team_to_pop[team_idx].add(pop_idx)
                    self.member_to_pop[team_idx][member_idx].add(pop_idx)

                    self.pop_and_team_to_valid_locations[pop_idx][team_idx][member_idx] = True
        for pop_idx in range(len(self.population_sizes)):
            if not self.pop_to_member[pop_idx]:
                raise Exception("population", pop_idx,
                                "has no associated team member, revise team_size or member_to_population arg"
                                )
        self.pop_to_team = tuple(self.pop_to_team)
        self.pop_to_member = tuple(self.pop_to_member)

        self.team_to_pop = tuple(self.team_to_pop)
        self.member_to_pop = tuple(tuple(t) for t in self.member_to_pop)

        self.pop_and_team_to_valid_locations = tuple(tuple(t) for t in self.pop_and_team_to_valid_locations)

        pop_members = [set(range(self.cumsums[i], self.cumsums[i + 1]))
                       for i in range(len(self.population_sizes))]
        self.team_to_choices = []
        for team_idx in range(len(self.team_sizes)):
            choices = set()
            for pop_idx in self.team_to_pop[team_idx]:
                choices.update(pop_members[pop_idx])
            self.team_to_choices.append(choices)
        self.team_to_choices = tuple(self.team_to_choices)

        # for each team, is a (team members, self.N) array of which agents are valid choices
        self.team_to_valid_members = []
        for team_idx, team_size in enumerate(self.team_sizes):
            valid_members = torch.zeros((team_size, self.N), dtype=torch.bool)
            for member_idx in range(team_size):
                for pop_idx in self.member_to_pop[team_idx][member_idx]:
                    # every member drawn from this population is valid, so give it a 1
                    valid_members[member_idx, list(pop_members[pop_idx])] = 1
            self.team_to_valid_members.append(valid_members)

    def sample_team_member_from_pop(self, pop_idx):
        for idx in self.pop_to_member[pop_idx]:
            return idx

    def index_to_pop_index(self, global_idx):
        """
        returns the index of agent specified by i
        Args:
            global_idx:
        Returns:
            population index (which population), index in population
        """
        pop_idx = 0
        while global_idx - self.population_sizes[pop_idx] >= 0:
            global_idx -= self.population_sizes[pop_idx]
            pop_idx += 1
        return pop_idx, global_idx

    def pop_index_to_index(self, pop_local_idx):
        pop_idx, local_idx = pop_local_idx
        return self.cumsums[pop_idx] + local_idx

    def get_info(self, pop_local_idx):
        return dict()

    def create_random_captians(self):
        """
        Returns: iterable of edges (i,j) that is a matching on (0,...,N)
            if N is not divisible by number of teams, a set of agents are chosen twice
            othewise, each agent is chosen once
        """

        unused = set(range(self.N))
        # this will always terminate as long as every population is used by at least one team
        # since then the unused set will always drop by at least one each time
        while unused:
            captains = [None for _ in range(self.num_teams)]
            uniques = [None for _ in range(self.num_teams)]
            for team_idx in torch.randperm(self.num_teams):
                choices = self.team_to_choices[team_idx]

                unused_choices = choices.intersection(unused)
                if unused_choices:
                    # if there are unused agents, use them here
                    cap = np.random.choice(list(unused_choices))
                    unique = True
                    unused.remove(cap)
                else:
                    # otherwise, default to using a previous agent
                    cap = np.random.choice(list(choices))
                    unique = False
                captains[team_idx] = cap
                uniques[team_idx] = unique
            yield tuple(captains), tuple(uniques)
            # remove captains from unused

    def parallel_seq_split(self, all_items_to_save):
        return all_items_to_save, []

    def epoch(self,
              rechoose=True,
              save_epoch_info=True,
              pre_ep_dicts=None,
              update_epoch_infos=True,
              **kwargs,
              ):
        """
        one epoch of training
        Args:
            rechoose: whether to breed/mutate
            save_epoch_info: whether to save epoch info to info dict
            pre_ep_dicts: episode dictionaries, if predefined
            update_epoch_infos: whether to save results into self.epoch_infos
            **kwargs: sent to pre_episode_generation and complete_epoch_and_extra_training
        Returns:
            epoch info dict
        """
        epoch_info = {
            'epoch': self.epochs,
            'update_results': update_epoch_infos,
        }
        epoch_info['episodes'] = []
        if pre_ep_dicts is None:
            pre_ep_dicts = [self.pre_episode_generation(captian_choices=cap_choice,
                                                        unique=unq,
                                                        rechoose=rechoose,
                                                        save_epoch_info=save_epoch_info,
                                                        pre_ep_dicts=pre_ep_dicts,
                                                        update_epoch_infos=update_epoch_infos,
                                                        **kwargs,
                                                        )
                            for cap_choice, unq in self.create_random_captians()]
        for i, pre_ep_dict in enumerate(pre_ep_dicts):
            pre_ep_dict['ident'] = str(i)

        if self.processes > 0:
            par_pre_ep_dicts, seq_pre_ep_dicts = self.parallel_seq_split(pre_ep_dicts)
        else:
            par_pre_ep_dicts, seq_pre_ep_dicts = [], pre_ep_dicts
        # parallel run
        if len(par_pre_ep_dicts) > 1:
            # TODO: This does not work when training? (check tests/multiproc)
            print('parallelizing', len(par_pre_ep_dicts), 'episodes')
            with Pool(processes=self.processes) as pool:
                par_items_to_save = pool.map(train_episode, par_pre_ep_dicts)
        else:
            seq_pre_ep_dicts = seq_pre_ep_dicts + par_pre_ep_dicts
            par_items_to_save = []

        # seq runs
        seq_items_to_save = [train_episode(pre_episode_dict=pre_ep_dict) for pre_ep_dict in seq_pre_ep_dicts]

        all_items_to_save = seq_items_to_save + par_items_to_save
        for items_to_save in all_items_to_save:
            epoch_info['episodes'].append(items_to_save['episode_info'])

        self.complete_epoch_and_extra_training(all_items_to_save=all_items_to_save,
                                               epoch_info=epoch_info,
                                               rechoose=rechoose,
                                               save_epoch_info=save_epoch_info,
                                               pre_ep_dicts=pre_ep_dicts,
                                               update_epoch_infos=update_epoch_infos,
                                               **kwargs,
                                               )
        if rechoose:
            epoch_info['breeding'] = self.breed()
            epoch_info['mutation'] = self.mutate()

        if update_epoch_infos:
            self.info['epochs'] += 1
            if save_epoch_info:
                self.epoch_infos.append(epoch_info)

        return epoch_info

    def complete_epoch_and_extra_training(self, all_items_to_save, epoch_info, **kwargs):
        """
        completes epoch given all information about
        updates epoch_info with any relevant epoch info
        Args:
            all_items_to_save: list of outputs of self.train_choice
            epoch_info: dict of info to save about current state
        Returns: None
        """
        pass

    def breed(self):
        """
        terminates poorly performing agents and replaces them with clones
            can also replace through sexual reproduction, if that is possible

        also updates internal variables to reflect the new agents
        Returns:
            breeding info
        """
        raise NotImplementedError

    def mutate(self):
        """
        mutates/reinitializes agents at random
        Returns:
            mutation info
        """
        pass

    def pre_episode_generation(self, captian_choices, unique, teams=None, **kwargs):
        """
        takes a choice of team captians and genrates teams and other info for training
        Args:
            captian_choices: tuple of captian indices, one for each team
            unique: wheher each captian is the first unique captian
            teams: list of torch arrays. If not specified, generates them
        Returns: info to be passed to update_results
        """
        raise NotImplementedError

    @property
    def epochs(self):
        return self.info['epochs']

    @property
    def epoch_infos(self):
        return self.info['epoch_infos']


class CaptianCoevolution(CoevolutionBase):
    """
    coevolution where matches are played between team captains
        i.e. a team built around team captians  (t_1,t_2,...) (one for each team) will play each other
            the score of the game will be used to update the team captian's elo
        this way, the performance of a team built to support the captian will only update the elo of the captian

        in teams with 1 member, this is not relevant, and is simple coevolution
    """

    def __init__(self,
                 outcome_fn_gen,
                 population_sizes,
                 team_trainer: TeamTrainer = None,
                 clone_fn=None,
                 team_sizes=(1, 1),
                 elo_conversion=400/np.log(10),
                 captian_elo_update=32*np.log(10)/400,
                 team_member_elo_update=0*np.log(10)/400,
                 mutation_fn=None,
                 member_to_population=None,
                 processes=0,
                 depth_to_retry_result=None,
                 storage_dir=None,
                 ):
        """
        Args:
            outcome_fn_gen: constructs outcome_fn outcomes and do RL training
                ()-> OutcomeFn
            population_sizes:
            team_trainer: model to select teams given a captian, may be trained alongside coevolution
                in games with 1 captain, this mostly does nothing, and can just be the default TeamTrainer class
            team_sizes:
            clone_fn:
            elo_conversion: to keep calculations simple, we use a scaled version of elo where the probability of
                player a (rating Ra) winning against player b (rating Rb) is simply 1/(1+e^{Rb-Ra})
                    i.e. win probabilities are calculated with softmax([Ra,Rb])
                to convert between this and classic elo (where win probs are 1/(1+10^{(Rb'-Ra')/400}), we simply scale
                    by the factor 400/np.log(10)
                This value is only used for display purposes, and will not change behavior of the learning algorithm
            captian_elo_update: Ra'=Ra+elo_update*(Sa-Ea) (https://en.wikipedia.org/wiki/Elo_rating_system).
                We are using scaled elo, so keep this in mind.
                    a 'classic' elo update of 32 is equivalent to a scaled 32*log(10)/400
            team_member_elo_update: elo update to use for team members (default 0)
                should update the team members less than the captain
            mutation_fn:
            processes: if positive, uses multiprocessing
            depth_to_retry_result: (game outcome value -> max number of times to try for a different outcome)
                if not specified, always 0
            storage_dir: place to store files and such
        """
        super().__init__(outcome_fn_gen=outcome_fn_gen,
                         population_sizes=population_sizes,
                         team_sizes=team_sizes,
                         member_to_population=member_to_population,
                         processes=processes,
                         storage_dir=storage_dir,
                         )
        if team_trainer is None:
            team_trainer = TeamTrainer(num_agents=self.N)
        self.team_trainer = team_trainer
        if storage_dir is not None:
            self.set_storage_dir(storage_dir=storage_dir)
        self.clone_fn = clone_fn
        self.mutation_fn = mutation_fn
        if depth_to_retry_result is None:
            depth_to_retry_result = lambda _: 0
        self.info.update({
            COEVOLUTION_DICT_ELOS: torch.zeros(self.N),
            COEVOLUTION_DICT_CAPTIAN_ELO_UPDATE: captian_elo_update,
            COEVOLUTION_DICT_MEMBER_ELO_UPDATE: team_member_elo_update,
            COEVOLUTION_DICT_ELO_CONVERSION: elo_conversion,
            COEVOLUTION_DICT_DEPTH_OF_RETRY: depth_to_retry_result,
        })

    def set_storage_dir(self, storage_dir):
        super().set_storage_dir(storage_dir=storage_dir)
        self.team_trainer.set_storage_dir(storage_dir=os.path.join(storage_dir, 'team_trainer'))

    def epoch(self,
              rechoose=True,
              save_epoch_info=True,
              pre_ep_dicts=None,
              update_epoch_infos=True,
              noise_model=None,
              depth=0,
              known_obs=(None, None),
              save_trained_agents=True,
              save_into_team_buffer=True,
              **kwargs,
              ):
        """
        one epoch of training
        Args:
            rechoose: whether to breed/mutate
            save_epoch_info: whether to save epoch info to info dict
            pre_ep_dicts: episode dictionaries, if predefined
            update_epoch_infos: whether to save results into self.epoch_infos
            depth: current depth of self.epoch() calls
            known_obs: (team index, Playerinfo observation), if known
            noise_model: model to put noise into team selection
                map of distributions from (*,num_agents) -> (*,num_agents)
                    default identity
            **kwargs: sent to pre_episode_generation and complete_epoch_and_extra_training
        Returns:
            epoch info dict
        """
        return super().epoch(rechoose=rechoose,
                             save_epoch_info=save_epoch_info,
                             pre_ep_dicts=pre_ep_dicts,
                             update_epoch_infos=update_epoch_infos,
                             depth=depth,
                             known_obs=known_obs,
                             noise_model=noise_model,
                             **kwargs,
                             )

    def complete_epoch_and_extra_training(self, all_items_to_save, epoch_info, **kwargs):
        super().complete_epoch_and_extra_training(all_items_to_save=all_items_to_save,
                                                  epoch_info=epoch_info,
                                                  **kwargs,
                                                  )
        save_trained_agents = kwargs.get('save_trained_agents', True)
        save_into_team_buffer = kwargs.get('save_into_team_buffer', True)
        if save_trained_agents or save_into_team_buffer:
            for items_to_save in all_items_to_save:
                if save_trained_agents:
                    self.update_results(items_to_save=items_to_save)
                if save_into_team_buffer:
                    self.update_team_buffer(items_to_save=items_to_save)

        depth = kwargs.get('depth', 0)
        noise_model = kwargs.get('noise_model', None)
        known_idx, known_player_info = kwargs.get('known_obs', (None, None))
        # whether episodes carry over observations
        # making this true will make for way slower training, as it is more difficult to parallelize
        combine_episode_obs = kwargs.get('combine_episode_obs', False)

        epoch_info['elos'] = self.elos.cpu().numpy()

        pre_ep_dicts = []

        for items_to_save in all_items_to_save:
            team_outcomes = items_to_save['team_outcomes']
            teams = items_to_save['teams']
            for team_idx, (team_outcome, player_infos) in enumerate(team_outcomes):
                if depth >= self.depth_to_retry_result(team_outcome):
                    continue
                # get all player infos together, if applicable
                if team_idx == known_idx:
                    combined_obs = known_player_info
                else:
                    combined_obs = PlayerInfo()

                for pi in player_infos:
                    combined_obs = combined_obs.union_obs(other_player_info=pi,
                                                          combine=True,
                                                          )
                obs_preembed, obs_mask = combined_obs.get_data(reshape=True)
                new_team, _ = self.create_team(team_idx=team_idx,
                                               captian=None,
                                               noise_model=noise_model,
                                               obs_preembed=obs_preembed,
                                               obs_mask=obs_mask,
                                               )
                teams = [new_team if idx == team_idx else team
                         for idx, team in enumerate(teams)]
                pre_ep_dict = self.pre_episode_generation(captian_choices=[team[torch.randint(0, len(team), (1,))]
                                                                           for team in teams],
                                                          unique=[False for _ in self.team_sizes],
                                                          teams=teams,
                                                          noise_model=None,
                                                          )
                if combine_episode_obs:
                    self.epoch(rechoose=False,
                               save_epoch_info=False,
                               pre_ep_dicts=[pre_ep_dict],
                               update_epoch_infos=False,
                               depth=depth + 1,
                               known_obs=(team_idx, combined_obs),
                               save_trained_agents=False,
                               save_into_team_buffer=True,
                               )
                else:
                    pre_ep_dicts.append(pre_ep_dict)
        if pre_ep_dicts:
            self.epoch(rechoose=False,
                       save_epoch_info=False,
                       pre_ep_dicts=pre_ep_dicts,
                       update_epoch_infos=False,
                       depth=depth + 1,
                       known_obs=(None, None),
                       save_trained_agents=False,
                       save_into_team_buffer=True,
                       )

    def clear(self):
        super().clear()
        self.team_trainer.clear()

    def save(self, save_dir):
        super().save(save_dir=save_dir)
        self.team_trainer.save(save_dir=os.path.join(save_dir, 'team_trainer'))

    def load(self, save_dir):
        super().load(save_dir=save_dir)
        self.team_trainer.load(save_dir=os.path.join(save_dir, 'team_trainer'))

    def create_team(self,
                    team_idx,
                    captian=None,
                    obs_preembed=None,
                    obs_mask=None,
                    noise_model=None,
                    team_noise_model=None,
                    ):
        """
        creates team to play at specified index
        Args:
            team_idx: index to create team for
            captian: captian that must be in team (None if no such thing)
            obs_preembed: observations about environment from previous play
            obs_mask: observation mask
            noise_model: noise model to use for selection
            team_noise_model: noise model to use for team assignment
        Returns:
            team (team_size,) torch array, captian position (or None)
        """
        team_size = self.team_sizes[team_idx]
        captian_pos = None
        if captian is not None:
            captian_pop_idx, _ = self.index_to_pop_index(global_idx=captian)
            valid_locations = self.pop_and_team_to_valid_locations[captian_pop_idx][team_idx]
            team = self.team_trainer.add_member_to_team(member=captian,
                                                        T=team_size,
                                                        N=1,
                                                        team_noise_model=team_noise_model,
                                                        valid_locations=valid_locations.view((1, team_size)),
                                                        obs_preembed=obs_preembed,
                                                        obs_mask=obs_mask,
                                                        )
            captian_pos = torch.where(torch.eq(team.view(-1), captian))[0].item()
        else:
            team = self.team_trainer.create_masked_teams(T=team_size,
                                                         N=1,
                                                         )
        valid_members = self.team_to_valid_members[team_idx]
        team = self.team_trainer.fill_in_teams(initial_teams=team,
                                               noise_model=noise_model,
                                               valid_members=valid_members.view((1, team_size, self.N)),
                                               obs_preembed=obs_preembed,
                                               obs_mask=obs_mask,
                                               )
        team = team.detach().flatten()
        return team, captian_pos

    def pre_episode_generation(self, captian_choices, unique, teams=None, noise_model=None, **kwargs):
        """
        takes a choice of team captians and trains them in RL environment
        Args:
            captian_choices: tuple of self.num_teams indices
            unique: whether each captian is unique (each captian will be marked unique exactly once
            teams: list of torch arrays. If not specified, generates them with self.team_choice
        """
        episode_info = {'captian_choices': captian_choices,
                        'unique_captians': unique
                        }

        # team selection (pretty much does nothing if teams are size 1

        captian_positions = []
        if teams is None:
            teams = []
            for team_idx, (captian, team_size) in enumerate(zip(captian_choices, self.team_sizes)):
                team, captian_pos = self.create_team(team_idx=team_idx,
                                                     captian=captian,
                                                     noise_model=noise_model,
                                                     )
                captian_positions.append(captian_pos)
                teams.append(team)
        else:
            for team, captian in zip(teams, captian_choices):
                possible = torch.where(torch.eq(team.view(-1), captian))[0].flatten()
                captian_positions.append(possible[torch.randint(0, len(possible), (1,))])

        episode_info['teams'] = tuple(team.numpy() for team in teams)
        outcome_fn = self.create_outcome_fn()
        train_infos = []
        for team_idx, (captian_pos, unq, team) in enumerate(zip(captian_positions, unique, teams)):
            tinfo = []
            for i, member in enumerate(team):
                global_idx = member.item()
                captian_pop_idx, local_idx = self.index_to_pop_index(global_idx=global_idx)

                info = self.get_info(pop_local_idx=(captian_pop_idx, local_idx))
                if i == captian_pos:
                    info[TEMP_DICT_CAPTIAN] = True
                    info[TEMP_DICT_CAPTIAN_UNIQUE] = unq
                else:
                    info[TEMP_DICT_CAPTIAN] = False
                tinfo.append(info)
            train_infos.append(tinfo)

        env = self.env_constructor(train_infos)

        return {'ident': 0,
                'teams': teams,
                'agents': None,
                'train_infos': train_infos,
                'env': env,
                'outcome_fn': outcome_fn,
                'episode_info': episode_info,
                'captian_choices': captian_choices,
                }

    def update_results(self, items_to_save):
        # save trained agents, if applicable
        self.save_trained_agents(items_to_save['outcome_local_mem'])

        # expected win probabilities, assuming the teams win probability is determined by captian elo
        expected_win_probs = torch.softmax(self.elos[items_to_save['captian_choices'],],
                                           dim=-1)
        for (captian,
             team,
             (team_outcome, _),
             expected_outcome,
             ) in zip(items_to_save['captian_choices'],
                      items_to_save['teams'],
                      items_to_save['team_outcomes'],
                      expected_win_probs):
            self.elos[captian] += self.captian_elo_update*(team_outcome - expected_outcome)
            for member in team:
                self.elos[member] += self.member_elo_update*(team_outcome - expected_outcome)

    def update_team_buffer(self, items_to_save):
        for (team,
             (team_outcome, player_infos),
             ) in zip(items_to_save['teams'],
                      items_to_save['team_outcomes']
                      ):
            # combine all player observations into one (can also just add each individual PlayerInfo)
            combined_obs = PlayerInfo()
            for player_info in player_infos:
                combined_obs = combined_obs.union_obs(other_player_info=player_info)
            obs_preembed, obs_mask = combined_obs.get_data(reshape=True)
            self.team_trainer.add_to_buffer(scalar=team_outcome,
                                            obs_preembed=obs_preembed,
                                            team=team.reshape((1, -1)),
                                            obs_mask=obs_mask,
                                            )

    def save_trained_agents(self, outcome_fn_local_mem):
        """
        saves trained agents, if applicable
        Args:
            outcome_fn_local_mem: output of OutcomeFn.pop_local_mem()
        """
        pass

    def breed(self):
        if self.clone_fn is None:
            return
        replacements = []
        k = 0
        for popsize in self.population_sizes:
            dist = torch.softmax(self.elos[k:k + popsize], dim=-1)
            # sample from this distribution with replacements
            rep_sample = torch.multinomial(dist, popsize, replacement=True)
            rep_sample += k  # shift the indices to the start of this population
            replacements = replacements + [t.item() for t in rep_sample]

            k += popsize

        self.clone_fn(torch.arange(self.N), replacements)
        temp_arr = self.elos.clone()
        self.elos[np.arange(self.N)] = temp_arr[replacements]
        self.rebase_elos()
        return

    def mutate(self):
        if self.mutation_fn is not None:
            return self.mutation_fn()
        else:
            return super().mutate()

    def _get_rebased_elos(self, elos, base_elo):
        """
        scales all elos so that base_elo is average
        Args:
            elos: elos to rescale
            base_elo: elo to make the 'average' elo
        Returns: rebased elos
        """
        return elos + (base_elo - torch.mean(elos))

    def rebase_elos(self, base_elo=0.):
        """
        scales all elos so that base_elo is average
            does not change any elo calculations, as they are all based on relative difference
        Args:
            base_elo: elo to make the 'average' elo
        """
        self.set_elos(self._get_rebased_elos(
            elos=self.elos,
            base_elo=base_elo,
        )
        )

    @property
    def classic_elos(self):
        return self.get_classic_elo(base_elo=1000)

    def get_classic_elo(self, base_elo=None):
        """
        gets 'classic' elo values
        Args:
            base_elo: if specified, rebases the elos so this is the average elo
        Returns:
            'classic' elos, such that for players with 'classic' elos Ra', Rb', the win probability of a is
                1/(1+e^{(Rb'-Ra')/self.info['elo_conversion']})
            i.e. if the default elo_conversion is used, this is  1/(1+10^{(Rb'-Ra')/400}), the standard value
        """

        scaled_elos = self.elos*self.elo_conversion
        if base_elo is not None:
            scaled_elos = self._get_rebased_elos(elos=scaled_elos, base_elo=base_elo)
        return scaled_elos

    def get_inverted_distribution(self, elos):
        """
        gets distribution according to inverted elos (i.e. more likely to pick bad agents)
            currently picks according to softmax(-elos)
                this has the effect of choosing each agent proportional to 1/(win probability)

            another method would be to pick proportional to 1-softmax(elos)
                however, this method does not differentiate well between 'bad' and 'very bad' agents
        Args:
            elos: elos to convert into a distribution
        Returns:
            normalized distribution
        """
        return torch.softmax(-elos, dim=-1)

    @property
    def ages(self):
        return torch.tensor([self.get_info(pop_local_idx=self.index_to_pop_index(global_idx=global_idx)
                                           ).get(DICT_AGE, 0)
                             for global_idx in range(self.N)
                             ],
                            dtype=torch.float
                            )

    def set_elos(self, elos):
        self.info[COEVOLUTION_DICT_ELOS] = elos

    @property
    def elos(self):
        # note: since this returns the reference to a tensor
        # this can only be used to mutate indices
        # i.e. self.captian_elos[2]=3 mutates self.captian_elos
        # self.captian_elos=torch.rand(2) does not mutate captian_elos
        return self.info[COEVOLUTION_DICT_ELOS]

    @property
    def captian_elo_update(self):
        return self.info[COEVOLUTION_DICT_CAPTIAN_ELO_UPDATE]

    @property
    def member_elo_update(self):
        return self.info[COEVOLUTION_DICT_MEMBER_ELO_UPDATE]

    @property
    def elo_conversion(self):
        return self.info[COEVOLUTION_DICT_ELO_CONVERSION]

    def depth_to_retry_result(self, outcome_value):
        return self.info[COEVOLUTION_DICT_DEPTH_OF_RETRY](outcome_value)


class PettingZooCaptianCoevolution(CaptianCoevolution):
    """
    keeps track of a set of stable baseline algorithms, and uses them to
    """

    def __init__(self,
                 env_constructor,
                 outcome_fn_gen,
                 population_sizes,
                 storage_dir,
                 worker_constructors,
                 member_to_population=None,
                 team_trainer: TeamTrainer = None,
                 team_sizes=(1, 1),
                 elo_conversion=400/np.log(10),
                 captian_elo_update=32*np.log(10)/400,
                 team_member_elo_update=0*np.log(10)/400,
                 reinit_agents=True,
                 mutation_prob=.001,
                 protect_new=20,
                 clone_replacements=None,
                 team_idx_to_agent_id=None,
                 processes=0,
                 temp_zoo_dir=None,
                 max_steps_per_ep=float('inf'),
                 depth_to_retry_result=None,
                 ):
        """
        Args:
            env_constructor: train_infos -> environment
                train_infos is a list of teams, each team is a list of train info dicts (corresponding to players)
                usually train_infos is ignored, unless we are changing the environment wrt the ages of players or something
            outcome_fn_gen: constructs outcome_fn outcomes and do RL training
                ()-> PettingZooOutcomeFn
            population_sizes: list of K ints, size of each population
            team_trainer:
            worker_constructors: if specified, list of K functions, size of population
                each funciton goes from (index, enviornment) into
                    (a worker with the specified action and obs space, worker_info) tuple
                info dicts keys are found in unstable_baselines3.utils.dict_keys

                worker_info relevant keys:
                    DICT_IS_WORKER: whether agent is a worker class
                    DICT_CLONABLE: whether agent is able to be cloned
                    DICT_CLONE_REPLACABLE: whether agent is able to be replace by a clone of another agent
                    DICT_MUTATION_REPLACABLE: whether agent is resettable in mutation
                    DICT_KEEP_OLD_BUFFER: whether to keep old buffer in event of reset
                    DICT_POSITION_DEPENDENT: set of dict that is associated with position:
                        i.e. if any entries are in this set, they will be unchanged after replacing agent with a clone
                            or mutation
            storage_dir: place to store files and such
            team_sizes:
            elo_conversion:
            captian_elo_update:
            mutation_prob: probability an agent randomly reinitializes each epoch
            clone_replacements: max number of agents to replace with clones each epoch
                if None, replaces all potentially
            protect_new: protect agents younger than this
            processes: if positive, uses multiprocessing
            temp_zoo_dir: place to store temp files
                if None, uses zoo_dir/temp_cage
            max_steps_per_ep: maximum steps per episode, used to decide whether to parallelize
            depth_to_retry_result: (game outcome value -> max number of times to try for a different outcome)
                if not specified, always 0
        """
        super().__init__(outcome_fn_gen=outcome_fn_gen,
                         population_sizes=population_sizes,
                         team_trainer=team_trainer,
                         team_sizes=team_sizes,
                         elo_conversion=elo_conversion,
                         captian_elo_update=captian_elo_update,
                         team_member_elo_update=team_member_elo_update,
                         mutation_fn=None,
                         clone_fn=None,
                         member_to_population=member_to_population,
                         processes=processes,
                         depth_to_retry_result=depth_to_retry_result,
                         storage_dir=storage_dir,
                         )

        self.env_constructor = env_constructor

        test_env = self.env_constructor(None)
        test_env.reset()

        if team_idx_to_agent_id is not None:
            self.team_idx_to_agent_id = team_idx_to_agent_id
        else:
            env_agents = iter(test_env.agents)
            dict_team_idx_to_agent_id = dict()
            for team_idx, team_size in enumerate(self.team_sizes):
                for member_idx in range(team_size):
                    dict_team_idx_to_agent_id[(team_idx, member_idx)] = next(env_agents)
            self.team_idx_to_agent_id = lambda idx: dict_team_idx_to_agent_id[idx]

        self.action_space = test_env.action_space
        self.observation_space = test_env.observation_space

        def pop_idx_to_dumenv(pop_idx):
            idx = self.sample_team_member_from_pop(pop_idx=pop_idx)
            agent_id = self.team_idx_to_agent_id(idx=idx)
            return DumEnv(action_space=self.action_space(agent_id),
                          obs_space=self.observation_space(agent_id),
                          )

        self.worker_constructors = lambda pop_idx: (lambda i:
                                                    worker_constructors[pop_idx](i,
                                                                                 env=pop_idx_to_dumenv(pop_idx)
                                                                                 )
                                                    )
        # self.worker_constructors is now (pos_idx -> (local idx -> agent))
        zoo_dir = os.path.join(storage_dir, 'zoo')
        self.zoo = [
            ZooCage(zoo_dir=os.path.join(zoo_dir, 'cage_' + str(i)),
                    overwrite_zoo=True,
                    )
            for i in range(len(self.population_sizes))
        ]

        self.zoo_dir = zoo_dir
        self.temp_zoo_dir = temp_zoo_dir
        if self.temp_zoo_dir is None:
            self.temp_zoo_dir = os.path.join(zoo_dir, 'temp_cage')
        if not os.path.exists(self.temp_zoo_dir):
            os.makedirs(self.temp_zoo_dir)
        if reinit_agents:
            self.init_agents()
        self.set_mutation_prob(mutation_prob=mutation_prob)
        self.set_protect_new(protect_new=protect_new)
        self.set_clone_replacements(clone_replacements=clone_replacements)
        self.set_max_steps_per_ep(max_steps_per_ep=max_steps_per_ep)

    def complete_epoch_and_extra_training(self, all_items_to_save, epoch_info, **kwargs):
        super().complete_epoch_and_extra_training(all_items_to_save=all_items_to_save,
                                                  epoch_info=epoch_info,
                                                  **kwargs,
                                                  )
        epoch_info['agent_classes'] = tuple(
            type(
                self.load_animal(
                    pop_local_idx=self.index_to_pop_index(
                        global_idx=global_idx
                    ),
                    load_buffer=False
                )[0]
            )
            for global_idx in range(self.N))

    @property
    def max_steps_per_ep(self):
        return self.info['max_steps_per_ep']

    def set_max_steps_per_ep(self, max_steps_per_ep):
        self.info['max_steps_per_ep'] = max_steps_per_ep

    @property
    def clone_replacements(self):
        return self.info['clone_replacements']

    def set_clone_replacements(self, clone_replacements):
        self.info['clone_replacements'] = clone_replacements

    @property
    def mutation_prob(self):
        return self.info['mutation_prob']

    def set_mutation_prob(self, mutation_prob):
        self.info['mutation_prob'] = mutation_prob

    @property
    def protect_new(self):
        return self.info['protect_new']

    def set_protect_new(self, protect_new):
        self.info['protect_new'] = protect_new

    def create_outcome_fn(self):
        """
        creates outcome function using self.outcome_fn_gen
            this version also tells PettingZooOutcomeFn where our self.zoo is and what our idx conversion is
        Returns: PettingZooOutcomeFn
        """
        outcome_fn = super().create_outcome_fn()
        outcome_fn: PettingZooOutcomeFn
        outcome_fn.set_zoo_dirs_and_pop_sizes(zoo_dirs=[zoocage.zoo_dir for zoocage in self.zoo],
                                              population_sizes=self.population_sizes,
                                              )
        outcome_fn.set_dir(self.temp_zoo_dir)
        return outcome_fn

    def index_to_agent(self, idx, training_dict):
        """
        takes index and agent training dict and returns agent
            assumes self.set_zoo and self.set_index_conversion have already been called
        Args:
            idx: global index of agent
            training_dict: training dictionary associated with agent
                includes whether agent is captian, whether there are repeats, etc

        Returns:

        """
        pop_idx, local_idx = self.index_to_pop_index(idx)
        collect_only = training_dict.get(DICT_COLLECT_ONLY, False)

        agent, saved_info = self.zoo[pop_idx].load_animal(key=str(local_idx),
                                                          load_buffer=not collect_only,
                                                          )
        return agent

    def pre_episode_generation(self, captian_choices, unique, teams=None, noise_model=None, **kwargs):
        pre_ep_dict = super().pre_episode_generation(captian_choices=captian_choices,
                                                     unique=unique,
                                                     teams=teams,
                                                     noise_model=noise_model,
                                                     )
        team_choices, updated_train_infos = pre_ep_dict['teams'], pre_ep_dict['train_infos']
        pre_ep_dict.update({'agents': [
            [self.index_to_agent(member.item(), member_training) for member, member_training in zip(*t)]
            for t in zip(team_choices, updated_train_infos)]})
        return pre_ep_dict

    def save_trained_agents(self, outcome_fn_local_mem):
        """
        saves trained agents
        Args:
            outcome_fn_local_mem: output of PettingZooOutcomeFn.pop_local_mem()
                is a dict of (global idx -> (trained agent, updated agent info))
        """
        for global_idx in outcome_fn_local_mem:
            for agent_dir, agent, updated_train_dict in outcome_fn_local_mem[global_idx]:
                pop_idx, local_idx = self.index_to_pop_index(global_idx)
                cage: ZooCage = self.zoo[pop_idx]
                if updated_train_dict.get(DICT_IS_WORKER, True):
                    if agent is None:
                        load_worker(save_dir=agent_dir,
                                    WorkerClass=None,
                                    load_buffer=updated_train_dict.get(DICT_SAVE_BUFFER, True),
                                    )

                    if updated_train_dict.get(DICT_COLLECT_ONLY, False):
                        cage.update_worker_buffer(local_worker=agent,
                                                  worker_key=str(local_idx),
                                                  WorkerClass=None,
                                                  )
                    elif updated_train_dict.get(DICT_TRAIN, True):
                        cage.overwrite_worker(worker=agent,
                                              worker_key=str(local_idx),
                                              save_buffer=updated_train_dict.get(DICT_SAVE_BUFFER, True),
                                              save_class=updated_train_dict.get(DICT_SAVE_CLASS, True),
                                              worker_info=updated_train_dict,
                                              )
                cage.overwrite_info(key=str(local_idx), info=updated_train_dict)
                if updated_train_dict[TEMP_DICT_CAPTIAN]:
                    # TODO: right now switching agents immediately after seeing captian, probably can do better
                    break

    def clear(self):
        super().clear()
        shutil.rmtree(self.temp_zoo_dir)
        self.clear_zoo()

    def save(self, save_dir):
        super().save(save_dir=save_dir)
        self.save_zoo(save_dir=os.path.join(save_dir, 'zoo'))

    def load(self, save_dir):
        super().load(save_dir=save_dir)
        self.load_zoo(save_dir=os.path.join(save_dir, 'zoo'))

    def init_agents(self):
        for pop_idx, popsize in enumerate(self.population_sizes):
            for local_idx in range(popsize):
                self.reset_agent(pop_local_idx=(pop_idx, local_idx),
                                 elo=torch.mean(self.elos),
                                 )

    def reset_agent(self, pop_local_idx, elo=None):
        pop_idx, local_idx = pop_local_idx
        agent, info = self.worker_constructors(pop_idx)(local_idx)
        cage = self.zoo[pop_idx]
        cage.overwrite_animal(animal=agent,
                              key=str(local_idx),
                              info=info,
                              save_buffer=True,
                              save_class=True,
                              )
        if elo is not None:
            self.elos[self.pop_index_to_index(pop_local_idx=pop_local_idx)] = elo

    def get_info(self, pop_local_idx):
        pop_idx, local_idx = pop_local_idx
        return self.zoo[pop_idx].load_info(key=str(local_idx))

    def breed(self):
        if self.clone_replacements is None:
            return self.classic_breed()
        else:
            return self.conservative_breed(number_to_replace=self.clone_replacements,
                                           )

    def _get_valid_idxs(self, validity_fn, indices=None):
        """
        returns indexes of all 'valid agents' according to check
        Args:
            validity_fn: (info dict -> whether to return agent)
            indices: indices to filter (if None, uses all indices)
        Returns:
            iterable of global indexes of replacable agents
        """
        if indices is None:
            indices = range(self.N)

        for global_idx in indices:
            pop_local_idx = self.index_to_pop_index(global_idx=global_idx)
            info = self.get_info(pop_local_idx=pop_local_idx)
            if validity_fn(info):
                yield global_idx

    def conservative_breed(self, number_to_replace: [int], base_elo=0., force_replacements=False):
        """
        Args:
            number_to_replace: max number of agents to try to replace in each population (must be in range [0,self.N])
                if iterable, must be of size (number of populations)
            base_elo: if not None, rebases all elos so this value is average
            force_replacements: if True, always replaces number_to_replace
                (unless there are fewer potential target agents than number_to_replace)
        keeps rest of the agents constant
            selects agents according to softmax of inverted ELO
                i.e. takes negative of the ELO and uses that to select 'bad' agents to replace
            tries to replace this agent, (checks DICT_AGE and DICT_IS_CLONE_REPLACABLE)

            chooses replacements with softmax of standard ELO
        """
        breed_dic = {'number_replaced': [0 for _ in self.population_sizes],
                     'target_agents': [],
                     'target_elos': [],
                     'cloned_agents': [],
                     'cloned_elos': [],
                     }

        if type(number_to_replace) == int:
            number_to_replace = [number_to_replace for _ in self.population_sizes]
        k = 0
        for pop_idx, popsize in enumerate(self.population_sizes):
            if number_to_replace[pop_idx] <= 0:
                continue

            # pick the agents to potentially clone
            candidate_clone_idxs = list(self._get_valid_idxs(validity_fn=
                                                             lambda info:
                                                             info.get(DICT_CLONABLE, True),
                                                             indices=range(k, k + popsize),
                                                             )
                                        )
            if not candidate_clone_idxs:
                # no clonable agents
                continue

            # pick the agents to potentially replace with a clone
            if force_replacements:
                candidate_target_idxs = list(
                    self._get_valid_idxs(validity_fn=lambda info: info.get(DICT_CLONE_REPLACABLE, True) and
                                                                  (info.get(DICT_AGE, 0) > self.protect_new),
                                         indices=range(k, k + popsize),
                                         ))
            else:
                candidate_target_idxs = list(range(k, k + popsize))
            # can replace at most this number
            number_to_replace[pop_idx] = min(len(candidate_target_idxs), number_to_replace[pop_idx])
            if not candidate_target_idxs:
                continue
            # distribution of agents based on how bad they are
            candidate_target_dist = self.get_inverted_distribution(elos=self.elos[candidate_target_idxs])
            # pick a random subset of target agents to replace based on this distribution
            target_idx_idxs = torch.multinomial(candidate_target_dist, number_to_replace[pop_idx], replacement=False)
            # these are the global indexes of the targets
            target_global_idxs = [candidate_target_idxs[target_idx_idx] for target_idx_idx in target_idx_idxs]
            target_elos = [self.elos[target_global_idx].item() for target_global_idx in target_global_idxs]

            # now pick which agents to clone based on elo
            candidate_clone_elos = self.elos[candidate_clone_idxs]
            clone_dist = torch.softmax(candidate_clone_elos, dim=-1)
            # sample from this distribution with replacement
            clone_idx_idxs = list(torch.multinomial(clone_dist, len(target_global_idxs), replacement=True))
            # element clone_idx_idx in clone_idx_idxs denotes that candidate_clone_idxs[clone_idx_idx] should be cloned
            # also candidate_clone_idxs[clone_idx_idx] has elo clone_elos[clone_idx_idx]

            # global indexes of clones, as well as elos of the clones
            clone_global_idxs = [candidate_clone_idxs[clone_idx_idx] for clone_idx_idx in clone_idx_idxs]
            clone_elos = [candidate_clone_elos[clone_idx_idx].item() for clone_idx_idx in clone_idx_idxs]

            for target_global_idx, target_elo, clone_global_idx, clone_elo in zip(target_global_idxs,
                                                                                  target_elos,
                                                                                  clone_global_idxs,
                                                                                  clone_elos):
                target_info = self.get_info(pop_local_idx=self.index_to_pop_index(global_idx=target_global_idx))
                # check if target is actually replacable by a clone
                if target_info.get(DICT_CLONE_REPLACABLE, True) and (target_info.get(DICT_AGE, 0) > self.protect_new):
                    # in that case, replace target with clone
                    clone_pop_local_idx = self.index_to_pop_index(global_idx=clone_global_idx)
                    clone_agent, clone_info = self.load_animal(pop_local_idx=clone_pop_local_idx, load_buffer=True)
                    self.replace_agent(pop_local_idx=self.index_to_pop_index(global_idx=target_global_idx),
                                       replacement=(clone_agent, clone_info),
                                       elo=clone_elo,
                                       keep_old_buff=clone_info.get(DICT_KEEP_OLD_BUFFER, False),
                                       update_with_old_buff=clone_info.get(DICT_UPDATE_WITH_OLD_BUFFER, True),
                                       )
                    breed_dic['number_replaced'][pop_idx] += 1
                    breed_dic['target_agents'].append(target_global_idx)
                    breed_dic['cloned_agents'].append(clone_global_idx)
                    breed_dic['cloned_elos'].append(clone_elo)
                    breed_dic['target_elos'].append(target_elo)
                    # note: it is probably necessary to save clone_elo and target_elo lists beforehand as self.captain_elos
                    # are being reassigned with self.replace_agent
            k += popsize
        breed_dic['based_elos'] = base_elo
        if base_elo is not None:
            self.rebase_elos(base_elo=base_elo)
        self.age_up_all_agents()
        return breed_dic

    def load_animal(self, pop_local_idx, load_buffer=True):
        pop_idx, local_idx = pop_local_idx
        return self.zoo[pop_idx].load_animal(key=str(local_idx),
                                             load_buffer=load_buffer,
                                             )

    def classic_breed(self, base_elo=0.):
        """
        breeds according to softmax selection
        https://ieeexplore.ieee.org/document/9308290
        """
        # equivalent to conservative_breed with targets being all agents
        return self.conservative_breed(number_to_replace=self.N,
                                       base_elo=base_elo,
                                       force_replacements=True,
                                       )

    def mutate(self):
        if self.mutation_prob == 0:
            return {'num_mutated': 0}
        mutation_dict = dict()

        mutatable_idxs = list(
            self._get_valid_idxs(validity_fn=lambda info: info.get(DICT_MUTATION_REPLACABLE, True) and
                                                          (info.get(DICT_MUTATION_AGE, 0) > self.protect_new)
                                 ))
        # get the number to mutate, choose each with probability self.mutation_prob
        # equivalent to the sum of a bunch of bernoulli variables
        num_to_mutate = int(torch.sum(
            torch.bernoulli(torch.tensor([self.mutation_prob for _ in mutatable_idxs]))
        ).item())
        mutation_dict['num_mutated'] = num_to_mutate
        if num_to_mutate > 0:
            # distribution based on how bad each agent is
            mut_dist = self.get_inverted_distribution(elos=self.elos[mutatable_idxs])
            # chose which to mutate without replacement
            mut_idx_idxs = torch.multinomial(mut_dist, num_to_mutate, replacement=False)
            # mutatable_idxs[mut_idx_idx] is the global index of an agent to mutate
            mut_global_idxs = [mutatable_idxs[mut_idx_idx] for mut_idx_idx in mut_idx_idxs]
            mutation_dict['idxs_mutated'] = mut_global_idxs

            elo_replacement = torch.mean(self.elos).item()
            mutation_dict['elo_replacement'] = elo_replacement
            for mut_global_idx in mut_global_idxs:
                mut_pop_local_idx = self.index_to_pop_index(global_idx=mut_global_idx)

                info = self.get_info(pop_local_idx=mut_pop_local_idx)
                mut_pop_idx, mut_local_idx = mut_pop_local_idx
                self.replace_agent(pop_local_idx=mut_pop_local_idx,
                                   replacement=self.worker_constructors(mut_pop_idx)(mut_local_idx),
                                   keep_old_buff=info.get(DICT_KEEP_OLD_BUFFER, False),
                                   update_with_old_buff=info.get(DICT_UPDATE_WITH_OLD_BUFFER, True),
                                   elo=elo_replacement,
                                   )
        return mutation_dict

    def replace_agent(self,
                      pop_local_idx,
                      replacement,
                      keep_old_buff=False,
                      update_with_old_buff=True,
                      elo=None,
                      mutated=False,
                      ):
        """
        replaces agent at specified idx with replacement agent
        Args:
            pop_local_idx: pop_idx,local_idx of agent to replace
            replacement: (agent, agent info) to replace agent with
                note that specified entries of info of old agent are carried over
                    specifically, the entries in info[DICT_POSITION_DEPENDENT]
            keep_old_buff: whether to just keep the buffer of old agent
                only works if both both agents share a buffer type
                    (i.e. both OnPolicyAlgorithm or both OffPolicyAlgorithm)
            update_with_old_buff: whether to update replacement agent's buffer with old agent
                only works if both both agents share a buffer type
                    (i.e. both OnPolicyAlgorithm or both OffPolicyAlgorithm)
            elo: elo to set new agent to
            mutated: whether agent was mutated
        """
        replacement_agent, replacement_info = replacement
        old_worker = None
        (pop_idx, local_idx) = pop_local_idx
        if keep_old_buff or update_with_old_buff:
            old_worker, _ = self.load_animal(pop_local_idx=pop_local_idx, load_buffer=True)
            # if the types of the old worker and replacement worker buffers are not the same, we cannot load old buffer
            if not (
                    (isinstance(old_worker, OnPolicyAlgorithm) and
                     isinstance(replacement_agent, OnPolicyAlgorithm) and
                     type(replacement_agent.rollout_buffer) == type(old_worker.rollout_buffer)
                    ) or
                    (isinstance(old_worker, OffPolicyAlgorithm) and
                     isinstance(replacement_agent, OffPolicyAlgorithm) and
                     type(replacement_agent.replay_buffer) == type(old_worker.replay_buffer)
                    )
            ):
                old_worker = None

        info = self.get_info(pop_local_idx=(pop_idx, local_idx))
        # keep these values
        position_dependent_keys = info.get(DICT_POSITION_DEPENDENT,
                                           # by default we want to keep replacability values
                                           {
                                               DICT_CLONE_REPLACABLE,
                                               DICT_MUTATION_REPLACABLE,
                                           },
                                           )
        position_dependent_keys.add(DICT_POSITION_DEPENDENT)

        replacement_info[DICT_AGE] = 0
        if mutated:
            replacement_info[DICT_MUTATION_AGE] = 0
        # update replacement dictionary with elements in position_dependent_keys dict
        for key in position_dependent_keys:
            if key in info:
                # copy the key over
                replacement_info[key] = info[key]
            elif key in replacement_info:
                # otherwise if key is in replacement info, remove it
                replacement_info.pop(key)

        self.zoo[pop_idx].overwrite_animal(animal=replacement_agent,
                                           key=str(local_idx),
                                           info=replacement_info,
                                           save_buffer=True,
                                           save_class=True,
                                           )
        if old_worker is not None:
            if keep_old_buff:
                # then clear the new buffer and update it with old_worker buffer
                self.zoo[pop_idx].clear_worker_buffer(worker_key=str(local_idx),
                                                      WorkerClass=None,
                                                      )
            self.zoo[pop_idx].update_worker_buffer(local_worker=old_worker,
                                                   worker_key=str(local_idx),
                                                   WorkerClass=None,
                                                   )
        if elo is not None:
            global_idx = self.pop_index_to_index(pop_local_idx=pop_local_idx)
            # replace elo as well
            self.elos[global_idx] = elo

    def save_zoo(self, save_dir):
        """
        saves all zoo cages to specified dir
        Args:
            save_dir: dir to save to
        """
        for zoo_cage in self.zoo:
            zoo_cage.save(save_dir=os.path.join(save_dir, os.path.basename(zoo_cage.zoo_dir)))

    def load_zoo(self, save_dir):
        """
        loads all zoo cages from specified dir
        Args:
            save_dir: dir to save to
        """
        for zoo_cage in self.zoo:
            zoo_cage.load(save_dir=os.path.join(save_dir, os.path.basename(zoo_cage.zoo_dir)))

    def clear_zoo(self):
        for zoo_cage in self.zoo:
            zoo_cage.clear()
        shutil.rmtree(self.zoo_dir)

    def age_up_all_agents(self):
        for pop_idx, popsize in enumerate(self.population_sizes):
            for local_idx in range(popsize):
                self.age_up_agent(pop_idx=pop_idx, local_idx=local_idx)

    def age_up_agent(self, pop_idx, local_idx):
        info = self.get_info(pop_local_idx=(pop_idx, local_idx))
        info[DICT_AGE] = info.get(DICT_AGE, 0) + 1
        info[DICT_MUTATION_AGE] = info.get(DICT_MUTATION_AGE, 0) + 1
        self.zoo[pop_idx].overwrite_info(key=str(local_idx), info=info)

    def parallel_seq_split(self, all_items_to_save):
        par_list, seq_list = [], []
        for items_to_save in all_items_to_save:
            agents = items_to_save['agents']
            updated_train_infos = items_to_save['train_infos']
            team_choices = items_to_save['teams']
            seq = False
            if agents is None:
                if agents is None:
                    agents = [
                        [self.index_to_agent(member.item(), member_training) for member, member_training in zip(*t)]
                        for t in zip(team_choices, updated_train_infos)]
            for t in zip(agents, updated_train_infos):
                for agent, train_info in zip(*t):
                    if train_info.get(DICT_IS_WORKER, True) and train_info.get(DICT_TRAIN, True):
                        if isinstance(agent, OffPolicyAlgorithm):
                            # off policy algs always train at the end of a rollout
                            capacity = 0
                        elif isinstance(agent, OnPolicyAlgorithm):
                            # if we can collect this episode without training, we can potentially do it in parallel
                            capacity = agent.rollout_buffer.buffer_size - agent.rollout_buffer.size()
                        else:
                            raise Exception("what alg is this", type(agent))
                        if self.max_steps_per_ep >= capacity:
                            seq = True
                            break
                if seq:
                    break
            if seq:
                seq_list.append(items_to_save)
            else:
                par_list.append(items_to_save)
        return par_list, seq_list


if __name__ == '__main__':
    import os, sys

    DIR = os.path.dirname(os.path.dirname(os.path.join(os.getcwd(), sys.argv[0])))

    torch.random.manual_seed(69)
    popsizes = [2, 2, 20]

    agents = torch.arange(sum(popsizes))%6


    def clone_fn(original, replacements):
        temp = agents.clone()
        agents[original] = temp[replacements]


    class MaxOutcome(PettingZooOutcomeFn):
        """
        return team with highest indices
        """

        def get_outcome(self, team_choices, agent_choices, updated_train_infos=None, env=None):
            agent_choices = [agents[team[0]] for team in team_choices]
            if agent_choices[0] == agent_choices[1]:
                return [(.5, []), (.5, [])]

            if agent_choices[0] > agent_choices[1]:
                return [(1, []), (0, [])]

            if agent_choices[0] < agent_choices[1]:
                return [(0, []), (1, [])]


    cap = CaptianCoevolution(outcome_fn_gen=MaxOutcome,
                             population_sizes=popsizes,
                             clone_fn=clone_fn
                             )
    for _ in range(20):
        for _ in range(2):
            cap.epoch(rechoose=False)
        print(cap.elos)
        print(agents)
        cap.epoch(rechoose=True)

    print(cap.elos)
    print(agents)

    from unstable_baselines3.dqn.DQN import WorkerDQN
    from stable_baselines3.dqn import MlpPolicy
    from pettingzoo.classic import tictactoe_v3


    def env_constructor(train_infos):
        return tictactoe_v3.env()


    capzoo = PettingZooCaptianCoevolution(env_constructor=env_constructor,
                                          outcome_fn_gen=MaxOutcome,
                                          population_sizes=[3, 4, 5],
                                          team_trainer=TeamTrainer(num_agents=3 + 4 + 5),
                                          worker_constructors=[lambda _, env: (WorkerDQN(policy=MlpPolicy,
                                                                                         env=env), {})
                                                               for _ in range(3)],
                                          zoo_dir=os.path.join(DIR, 'data', 'coevolver_zoo_test'),
                                          )
    capzoo.clear_zoo()
